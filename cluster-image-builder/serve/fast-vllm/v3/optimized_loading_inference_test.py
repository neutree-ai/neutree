#!/usr/bin/env python3
"""
ä¼˜åŒ–æ¨¡å‹åŠ è½½è„šæœ¬ - æµå¼é¦–Tokenæ—¶é—´æµ‹è¯•
æµ‹é‡ä»æ¨¡å‹åŠ è½½åˆ°é¦–ä¸ªtokenæµå‡ºçš„æ—¶é—´
"""

import torch
import time
import gc
import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from threading import Thread
from vllm.device_allocator.cumem import find_loaded_library
from vllm.distributed.device_communicators.cuda_wrapper import CudaRTLibrary

lib_name = find_loaded_library("cumem_allocator")
libcudart = CudaRTLibrary()

def human_size(nbytes):
    gb = 1024**3
    return f"{nbytes/gb:.2f} GB"

def gbps(nbytes, secs):
    return (nbytes / (1024**3)) / max(secs, 1e-9)

class StreamingTokenLoader:
    def __init__(self, model_path, device="cuda:0", batch_size=20):
        self.model_path = model_path
        self.device = device
        self.batch_size = batch_size
        self.tokenizer = None
        self.model = None
        
    def load_tokenizer(self):
        """åŠ è½½tokenizer"""
        print(f"=== åŠ è½½ Tokenizer ===")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, 
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        print(f"è¯æ±‡é‡: {len(self.tokenizer)}")
        return self.tokenizer
    
    def load_model_optimized(self):
        print(f"\n=== ä¼˜åŒ–åŠ è½½: {self.model_path} ===")

        # 1. CPU åŠ è½½
        t0 = time.perf_counter()
        cpu_model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            dtype=torch.float16,
            device_map="cpu",
            trust_remote_code=True
        )
        t1 = time.perf_counter()
        cpu_load_time = t1 - t0

        # 2. æ”¶é›†æƒé‡ï¼ˆä¸ benchmark ä¸€è‡´ï¼‰
        print("æ­¥éª¤2: é›¶æ‹·è´æƒé‡æå–...")
        t2 = time.perf_counter()
        params = list(cpu_model.parameters())
        cpu_weights = [p.detach() for p in params]                # <-- ä¿ç•™åˆ—è¡¨ï¼Œä¸åœ¨è¿™é‡Œ del
        total_bytes = sum(w.numel() * w.element_size() for w in cpu_weights)
        t3 = time.perf_counter()
        extract_time = t3 - t2

        print(f"CPUåŠ è½½: {cpu_load_time:.3f}s")
        print(f"æƒé‡æå–: {extract_time:.3f}s")
        print(f"æ¨¡å‹å¤§å°: {human_size(total_bytes)}")
        print(f"æƒé‡æ•°é‡: {len(cpu_weights)}")

        # 3. ç­‰å¾…ç”¨æˆ·ç¡®è®¤
        print("\nâ¸ï¸  å‡†å¤‡å¼€å§‹GPUä¼ è¾“")
        print("è¯·æ£€æŸ¥GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ (nvidia-smi)")
        while True:
            if input("\nç»§ç»­GPUä¼ è¾“? (y/n): ").strip().lower() == 'y':
                end_to_end_start_time = time.perf_counter()
                print("ğŸš€ å¼€å§‹ç«¯åˆ°ç«¯è®¡æ—¶ (åˆ°é¦–tokenæµå‡º)...")
                break
            else:
                print("ç”¨æˆ·å–æ¶ˆæˆ–è¾“å…¥é yï¼Œé€€å‡º")
                return None

        # 4. æ‰¹é‡ H2Dï¼ˆå¯¹é½ benchmarkï¼‰
        print(f"\næ­¥éª¤3: é«˜æ•ˆä¼ è¾“ (æ‰¹å¤§å°={self.batch_size})...")
        print(f"æƒé‡æ•°é‡: {len(cpu_weights)}")
        t4 = time.perf_counter()
        with torch.no_grad():
            for i in range(0, len(cpu_weights), self.batch_size):
                batch = cpu_weights[i:i+self.batch_size]
                for j, w in enumerate(batch):
                    g = w.to(self.device, non_blocking=True)
                    params[i + j].data = g
                torch.cuda.synchronize()
        t5 = time.perf_counter()
        transfer_time = t5 - t4
        bandwidth = gbps(total_bytes, transfer_time)
        print(f"ä¼ è¾“æ—¶é—´: {transfer_time:.3f}s")
        print(f"ä¼ è¾“å¸¦å®½: {bandwidth:.2f} GB/s")
        print(f"ç¡¬ä»¶æ•ˆç‡: {bandwidth/12.55*100:.1f}%")

        # 5. è½»é‡â€œé‡å»ºâ€
        t6 = time.perf_counter()
        device_obj = torch.device(self.device)
        cpu_model._device = device_obj
        for m in cpu_model.modules():
            setattr(m, "_device", device_obj)
        self.model = cpu_model
        t7 = time.perf_counter()
        rebuild_time = t7 - t6
        total_time = t7 - t0

        # === ä¸åœ¨è¿™é‡Œ delï¼›æŠŠ cpu_weights ç•™åˆ°é¦– token åå†æ¸…ç† ===
        # å­˜ä¸‹æ¥ä»¥ä¾¿åç»­å¼‚æ­¥æ¸…ç†ï¼ˆé¿å… TTFT è·¯å¾„è¢«é˜»å¡ï¼‰
        self._pending_cleanup = {
            "cpu_weights": cpu_weights,
            "params": params,
        }

        print("[[[return]]]", time.perf_counter() - end_to_end_start_time)

        return {
            'total_bytes': total_bytes,
            'cpu_load_time': cpu_load_time,
            'extract_time': extract_time,
            'transfer_time': transfer_time,
            'rebuild_time': rebuild_time,
            'total_time': total_time,
            'bandwidth': bandwidth,
            'end_to_end_start_time': end_to_end_start_time,
        }
        
    def _start_deferred_cleanup(self):
        payload = getattr(self, "_pending_cleanup", None)
        if not payload:
            return

        def _cleanup():
            try:
                cpu_weights = payload.get("cpu_weights") or []
                # åˆ†æ‰¹æ¸…ç©ºï¼Œé¿å…ä¸€æ¬¡æ€§é•¿åœé¡¿
                B = 512  # æ¯æ‰¹æ¬¡åˆ é™¤çš„ tensor æ•°ï¼›å¯æŒ‰æƒé‡æ•°é‡è°ƒå¤§/è°ƒå°
                for i in range(0, len(cpu_weights), B):
                    batch = cpu_weights[i:i+B]
                    # é€šè¿‡å°±åœ°ç½® None + æ¸…ç†åˆ‡ç‰‡å¼•ç”¨æ¥åŠ é€Ÿé‡Šæ”¾
                    for j in range(len(batch)):
                        batch[j] = None
                    # è®© Python æœ‰æœºä¼šåˆ‡æ¢
                    time.sleep(0)  # è®©å‡º GIL ç‰‡åˆ»ï¼Œé™ä½é•¿æ—¶é—´å ç”¨
                # æ¸…ç©ºåˆ—è¡¨æœ¬ä½“
                cpu_weights.clear()

                # è¿˜å¯ä»¥æ–­å¼€ params çš„å¼ºå¼•ç”¨ï¼ˆä¸æ˜¯å¿…é¡»ï¼‰
                params = payload.get("params")
                if params:
                    # ä¸è¦æ¸…ç©º .dataï¼ˆæ¨¡å‹è¿˜è¦ç”¨ï¼‰ï¼›è¿™é‡Œåªæ˜¯æŠŠ Python åˆ—è¡¨æœ¬èº«é‡Šæ”¾
                    payload["params"] = None

                # æœ€ååšä¸€æ¬¡ GC
                gc.collect()
            finally:
                # æ ‡è®°å®Œæˆï¼Œé¿å…é‡å¤æ‰§è¡Œ
                self._pending_cleanup = None

        t = Thread(target=_cleanup, daemon=True)
        t.start()
    
    def test_streaming_first_token(self, prompt, end_to_end_start_time):
        """æµ‹è¯•æµå¼ç”Ÿæˆçš„é¦–tokenæ—¶é—´"""
        print(f"\n=== æµå¼é¦–Tokenæµ‹è¯• ===")
        print(f"æµ‹è¯•æç¤º: {prompt}")
        
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½")
        
        self.model.eval()
        
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # åˆ›å»ºæµå¼ç”Ÿæˆå™¨
        streamer = TextIteratorStreamer(
            self.tokenizer, 
            skip_prompt=True, 
            skip_special_tokens=True
        )
        
        # å‡†å¤‡ç”Ÿæˆå‚æ•°
        generation_kwargs = {
            **inputs,
            'max_new_tokens': 50,
            'do_sample': False,
            'temperature': None,
            'top_p': None,
            'top_k': None,
            'pad_token_id': self.tokenizer.eos_token_id,
            'streamer': streamer
        }
        
        try:
            # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
            generation_thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
            
            # å¼€å§‹è®¡æ—¶å’Œç”Ÿæˆ
            stream_start = time.perf_counter()
            generation_thread.start()
            
            # ç­‰å¾…ç¬¬ä¸€ä¸ªtoken
            first_token_text = ""
            first_token_time = None
            stream_only_time = None
            
            for i, token_text in enumerate(streamer):
                if i == 0:  # ç¬¬ä¸€ä¸ªtoken
                    torch.cuda.synchronize()  # ç¡®ä¿GPUæ“ä½œå®Œæˆ
                    first_token_time = time.perf_counter() - end_to_end_start_time
                    stream_only_time = time.perf_counter() - stream_start
                    first_token_text = token_text
                    
                    print(f"ğŸ¯ ç«¯åˆ°ç«¯æ—¶é—´ (yç¡®è®¤â†’é¦–tokenæµå‡º): {first_token_time:.3f}s")
                    print(f"   çº¯ç”Ÿæˆæ—¶é—´: {stream_only_time:.3f}s")
                    print(f"   é¦–ä¸ªæµå‡ºtoken: '{first_token_text}'")
                    break
            
            # ç­‰å¾…ç”Ÿæˆçº¿ç¨‹å®Œæˆ
            generation_thread.join()
            
            if first_token_time is None:
                raise Exception("æœªèƒ½æ•è·é¦–token")
            
            return {
                'first_token_time': first_token_time,
                'stream_only_time': stream_only_time,
                'first_token_text': first_token_text,
                'method': 'streaming'
            }
            
        except Exception as e:
            print(f"âš ï¸ æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            print("å›é€€åˆ°å•tokenç”Ÿæˆ...")
            return self.test_single_token_fallback(prompt, end_to_end_start_time)
    
    def test_single_token_fallback(self, prompt, end_to_end_start_time):
        """å›é€€æ–¹æ¡ˆï¼šå•tokenç”Ÿæˆ"""
        print(f"\n=== å•Tokenç”Ÿæˆå›é€€ ===")
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            generation_start = time.perf_counter()
            
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=1,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            torch.cuda.synchronize()
            
            first_token_time = time.perf_counter() - end_to_end_start_time
            generation_only_time = time.perf_counter() - generation_start
            
            # æå–ç”Ÿæˆçš„token
            generated_token_id = outputs[0][-1].item()
            first_token_text = self.tokenizer.decode([generated_token_id], skip_special_tokens=True)
            
            print(f"ğŸ¯ ç«¯åˆ°ç«¯æ—¶é—´ (yç¡®è®¤â†’é¦–token): {first_token_time:.3f}s")
            print(f"   çº¯ç”Ÿæˆæ—¶é—´: {generation_only_time:.3f}s")
            print(f"   ç”Ÿæˆçš„é¦–token: '{first_token_text}'")
            
            return {
                'first_token_time': first_token_time,
                'generation_only_time': generation_only_time,
                'first_token_text': first_token_text,
                'method': 'single_token'
            }
    
    def test_regular_inference(self, prompts):
        """å¸¸è§„æ¨ç†æµ‹è¯•"""
        print(f"\n=== å¸¸è§„æ¨ç†æµ‹è¯• ===")
        
        results = []
        for i, prompt in enumerate(prompts):
            print(f"\næµ‹è¯• {i+1}: {prompt}")
            
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                t0 = time.perf_counter()
                
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=25,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
                torch.cuda.synchronize()
                
                t1 = time.perf_counter()
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            inference_time = t1 - t0
            new_tokens = outputs[0].shape[0] - inputs['input_ids'].shape[1]
            tokens_per_sec = new_tokens / inference_time if inference_time > 0 else 0
            
            print(f"æ¨ç†æ—¶é—´: {inference_time:.3f}s, é€Ÿåº¦: {tokens_per_sec:.1f} tok/s")
            print(f"è¾“å‡º: {response}")
            
            results.append({
                'inference_time': inference_time,
                'tokens_per_sec': tokens_per_sec,
                'new_tokens': new_tokens
            })
        
        return results
    
    def test_inference(self, load_stats):
        """æµ‹è¯•æ¨ç†å’Œç«¯åˆ°ç«¯æ€§èƒ½"""
        print(f"\n=== æ¨ç†æµ‹è¯• ===")
        
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½")
        
        # é¦–tokenæµ‹è¯•
        end_to_end_start_time = load_stats.get('end_to_end_start_time')
        first_token_result = None
        
        if end_to_end_start_time is not None:
            test_prompt = "What is artificial intelligence?"
            first_token_result = self.test_streaming_first_token(test_prompt, end_to_end_start_time)
        
        # å¸¸è§„æ¨ç†æµ‹è¯•
        regular_prompts = [
            "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
            "è®¡ç®— 8 * 7 = ",
            "å†™ä¸€ä¸ªPythonå‡½æ•°"
        ]
        
        regular_results = self.test_regular_inference(regular_prompts)
        
        return {
            'first_token_result': first_token_result,
            'regular_results': regular_results
        }
    
    def print_report(self, load_stats, inference_results):
        """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        print(f"\n=== æ€§èƒ½æŠ¥å‘Š ===")
        print(f"æ¨¡å‹: {self.model_path}")
        print(f"å¤§å°: {human_size(load_stats['total_bytes'])}")
        print(f"æ‰¹å¤§å°: {self.batch_size}")
        
        print(f"\nåŠ è½½æ€§èƒ½:")
        print(f"  ä¼ è¾“å¸¦å®½: {load_stats['bandwidth']:.2f} GB/s")
        print(f"  ç¡¬ä»¶æ•ˆç‡: {load_stats['bandwidth']/12.55*100:.1f}%")
        print(f"  æ€»åŠ è½½æ—¶é—´: {load_stats['total_time']:.3f}s")
        
        # é¦–tokenæ€§èƒ½åˆ†æ
        first_token_result = inference_results.get('first_token_result')
        if first_token_result:
            first_token_time = first_token_result['first_token_time']
            method = first_token_result.get('method', 'unknown')
            
            if method == 'streaming':
                pure_inference_time = first_token_result.get('stream_only_time', 0)
                inference_label = "çº¯æµå¼ç”Ÿæˆ"
            else:
                pure_inference_time = first_token_result.get('generation_only_time', 0)
                inference_label = "çº¯ç”Ÿæˆæ—¶é—´"
            
            print(f"\nğŸ¯ é¦–Tokenæ€§èƒ½ (å…³é”®æŒ‡æ ‡) - {method}æ¨¡å¼:")
            print(f"  ç«¯åˆ°ç«¯æ—¶é—´: {first_token_time:.3f}s")
            print(f"  {inference_label}: {pure_inference_time:.3f}s")
            print(f"  åŠ è½½+å¯åŠ¨å¼€é”€: {first_token_time - pure_inference_time:.3f}s")
            print(f"  ç”ŸæˆToken: '{first_token_result['first_token_text']}'")
            
            # åˆ†ææ€§èƒ½ç“¶é¢ˆ
            loading_phase = load_stats['total_time']
            startup_overhead = first_token_time - loading_phase - pure_inference_time
            
            print(f"\næ€§èƒ½åˆ†è§£:")
            print(f"  1. æ¨¡å‹åŠ è½½: {loading_phase:.3f}s ({loading_phase/first_token_time*100:.1f}%)")
            print(f"  2. å¯åŠ¨å¼€é”€: {startup_overhead:.3f}s ({startup_overhead/first_token_time*100:.1f}%)")
            print(f"  3. å®é™…æ¨ç†: {pure_inference_time:.3f}s ({pure_inference_time/first_token_time*100:.1f}%)")
            print(f"  æ€»è®¡: {first_token_time:.3f}s")
            
            if method == 'streaming':
                print(f"\nğŸ“¡ æµå¼ç”Ÿæˆä¼˜åŠ¿:")
                print(f"  - ç”¨æˆ·æ„ŸçŸ¥å»¶è¿Ÿ: {first_token_time:.3f}s (é¦–tokenå‡ºç°)")
                print(f"  - åç»­tokenå°†æŒç»­æµå‡º")
                print(f"  - æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ (æ— éœ€ç­‰å¾…å®Œæ•´å“åº”)")
        
        # å¸¸è§„æ¨ç†æ€§èƒ½
        regular_results = inference_results.get('regular_results', [])
        if regular_results:
            valid_results = [r for r in regular_results if r['tokens_per_sec'] > 0]
            if valid_results:
                avg_speed = sum(r['tokens_per_sec'] for r in valid_results) / len(valid_results)
                avg_time = sum(r['inference_time'] for r in valid_results) / len(valid_results)
                print(f"\nå¸¸è§„æ¨ç†æ€§èƒ½:")
                print(f"  å¹³å‡é€Ÿåº¦: {avg_speed:.1f} tokens/s")
                print(f"  å¹³å‡æ—¶é—´: {avg_time:.3f}s")
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆ")
        
        # ä¼˜åŒ–å»ºè®®
        if first_token_result:
            method = first_token_result.get('method', 'unknown')
            pure_inference_time = (first_token_result.get('stream_only_time') or 
                                 first_token_result.get('generation_only_time', 0))
            loading_phase = load_stats['total_time']
            startup_overhead = first_token_time - loading_phase - pure_inference_time
            
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            if startup_overhead > 0.1:
                print(f"  - å¯åŠ¨å¼€é”€è¾ƒå¤§({startup_overhead:.3f}s)ï¼Œè€ƒè™‘æ¨¡å‹é¢„çƒ­")
            if pure_inference_time > 0.05:
                print(f"  - æ¨ç†æ—¶é—´è¾ƒé•¿ï¼Œè€ƒè™‘é‡åŒ–æˆ–æ›´å°æ¨¡å‹")
            if loading_phase > first_token_time * 0.7:
                print(f"  - åŠ è½½æ—¶é—´å æ¯”å¤§ï¼Œæ¨¡å‹ä¼ è¾“å·²ä¼˜åŒ–")
            if method == 'streaming':
                print(f"  âœ… å·²ä½¿ç”¨æµå¼ç”Ÿæˆï¼Œç”¨æˆ·ä½“éªŒæœ€ä¼˜")
    
    def run_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print(f"=== æµå¼é¦–Tokenæ—¶é—´æµ‹è¯•: {self.model_path} ===")
        print(f"ğŸ“Š æµ‹é‡æŒ‡æ ‡ï¼šTTFT (Time to First Token)")
        print(f"ğŸ¯ ç›®æ ‡ï¼šç”¨æˆ·æ„ŸçŸ¥åˆ°çš„é¦–ä¸ªtokenæµå‡ºæ—¶é—´")
        
        # åŠ è½½tokenizer
        self.load_tokenizer()
        
        # ä¼˜åŒ–åŠ è½½æ¨¡å‹
        load_stats = self.load_model_optimized()
        print("[[[load_stats]]]", time.perf_counter() - load_stats['end_to_end_start_time'])
        # self._start_deferred_cleanup()
        if load_stats is None:
            print("ç”¨æˆ·å–æ¶ˆæµ‹è¯•")
            return
        
        # æ¨ç†æµ‹è¯•
        inference_results = self.test_inference(load_stats)
        
        # æ‰“å°æŠ¥å‘Š
        self.print_report(load_stats, inference_results)
        
        # ç­‰å¾…ç”¨æˆ·ç¡®è®¤æ¸…ç†
        print(f"\n{'='*50}")
        print("ğŸ” ç¬¬ä¸€è½®æµ‹è¯•å·²å®Œæˆ")
        print("ğŸ“Š è¯·æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ (nvidia-smi)")
        print("ğŸ’¡ ç¡®è®¤åå°†å¼€å§‹æ¸…ç†èµ„æºï¼Œæ‚¨å¯ä»¥è§‚å¯Ÿæ˜¾å­˜é‡Šæ”¾è¿‡ç¨‹")
        print(f"{'='*50}")
        
        # while True:
        #     user_input = input("\nç¡®è®¤å¼€å§‹æ¸…ç†? (y/n): ").strip().lower()
        #     if user_input == 'y':
        #         print("\nğŸ§¹ å¼€å§‹æ¸…ç†èµ„æº...")
        #         # æ‰§è¡Œæ¸…ç†æ“ä½œ
        #         self.cleanup()
        #         print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
                
        #         # è¯¢é—®æ˜¯å¦é‡æ–°è¿è¡Œæµ‹è¯•
        #         print(f"\n{'='*50}")
        #         print("ğŸ”„ æ˜¾å­˜å·²é‡Šæ”¾ï¼Œå¯ä»¥é‡æ–°è¿è¡Œæµ‹è¯•")
        #         print("ğŸ“Š è¯·å†æ¬¡æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ (nvidia-smi)")
        #         print(f"{'='*50}")
                
        #         while True:
        #             rerun_input = input("\né‡æ–°è¿è¡Œæµ‹è¯•? (y/n): ").strip().lower()
        #             if rerun_input == 'y':
        #                 print("\nğŸ”„ å¼€å§‹é‡æ–°è¿è¡Œæµ‹è¯•...")
        #                 # é‡æ–°åŠ è½½tokenizerå’Œæ¨¡å‹ï¼Œå†æ¬¡è¿è¡Œæµ‹è¯•
        #                 self.load_tokenizer()
        #                 rerun_load_stats = self.load_model_optimized()
        #                 if rerun_load_stats is not None:
        #                     rerun_inference_results = self.test_inference(rerun_load_stats)
        #                     self.print_report(rerun_load_stats, rerun_inference_results)
        #                     print("\nğŸ¯ ç¬¬äºŒè½®æµ‹è¯•å®Œæˆ")
        #                 return load_stats, inference_results
        #             elif rerun_input == 'n':
        #                 print("æµ‹è¯•ç»“æŸ")
        #                 return load_stats, inference_results
        #             else:
        #                 print("è¯·è¾“å…¥ y æˆ– n")
                        
        #     elif user_input == 'n':
        #         print("ç”¨æˆ·å–æ¶ˆæ¸…ç†ï¼Œä¿æŒå½“å‰çŠ¶æ€")
        #         return load_stats, inference_results
        #     else:
        #         print("è¯·è¾“å…¥ y æˆ– n")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        print("  æ­£åœ¨æ¸…ç†æ¨¡å‹...")
        if self.model is not None:
            del self.model
            self.model = None
        
        print("  æ­£åœ¨æ¸…ç†tokenizer...")
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
            
        print("  æ­£åœ¨æ¸…ç†ç¼“å­˜...")
        # æ¸…ç†å¾…å¤„ç†çš„cleanupä»»åŠ¡
        if hasattr(self, '_pending_cleanup'):
            self._pending_cleanup = None
            
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        print("Before reset:", torch.cuda.is_available())
        
        
        print("  æ­£åœ¨é‡ç½®CUDAè®¾å¤‡...")
        libcudart.cudaDeviceReset()
        
        # torch.cuda._lazy_init()
        # print("PyTorch CUDA reinitialized")
        
        print("After reset:", torch.cuda.is_available())
        
        
        print("  æ˜¾å­˜æ¸…ç†å®Œæˆ")

def main():
    parser = argparse.ArgumentParser(description="è¯¦ç»†æ€§èƒ½åˆ†æ - å®šä½TTFTç“¶é¢ˆç‚¹")
    parser.add_argument("--model", default="Qwen/Qwen3-14B", help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--device", default="cuda:0", help="GPUè®¾å¤‡")
    parser.add_argument("--batch-size", type=int, default=20, help="æ‰¹é‡å¤§å°")
    
    args = parser.parse_args()
    
    loader = StreamingTokenLoader(args.model, args.device, args.batch_size)
    
    try:
        loader.run_test()
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        loader.cleanup()

if __name__ == "__main__":
    main()