import os, time, gc, threading
import torch
import torch.nn as nn
from queue import Queue
from typing import List, Tuple
from transformers import AutoModelForCausalLM

from vllm.config import ModelConfig, VllmConfig, LoadConfig
from vllm.model_executor.model_loader import register_model_loader
from vllm.model_executor.model_loader.base_loader import BaseModelLoader
from vllm.model_executor.model_loader.utils import initialize_model, set_default_torch_dtype

import patch_v1_engine  # noqa: F401

# mock
from unittest.mock import patch
import vllm.distributed
import vllm.distributed.parallel_state as parallel_state

# os.environ.setdefault("VLLM_ENABLE_V1_MULTIPROCESSING", "0")

TARGET_DEVICE = torch.device(os.environ.get("MEMMAP_DEVICE", "cuda:0"))
BATCH_SIZE    = int(os.environ.get("MEMMAP_BATCH_SIZE", "20"))   # å¤ç”¨ loading test çš„æ‰¹é‡å¤§å°
FORCE_DTYPE   = os.environ.get("MEMMAP_DTYPE", "").strip().lower() or None

def human_size(nbytes):
    gb = 1024**3
    return f"{nbytes/gb:.2f} GB"

def gbps(nbytes, secs):
    return (nbytes / (1024**3)) / max(secs, 1e-9)

# å…¨å±€é¢„çƒ­çŠ¶æ€
_global_cpu_model = None
_global_cpu_weights = None
_global_total_bytes = 0
_global_preheated_path = None

class MockGroupCoordinator:
    @property
    def is_first_rank(self):
        return True

    @property
    def is_last_rank(self):
        return True

    @property
    def rank_in_group(self):
        return 0

    @property
    def world_size(self):
        return 1

# Mock å¹¶è¡Œç›¸å…³å‡½æ•°
def mock_get_tensor_model_parallel_rank():
    return 0

def mock_get_tensor_model_parallel_world_size():
    return 1


@register_model_loader("memmap")
class FastMemmapLoader(BaseModelLoader):
    def __init__(self, load_config: LoadConfig):
        super().__init__(load_config)
        print(f"[FastMemmapLoader] åˆ›å»ºå®ä¾‹: {id(self)}")

    def preheat(self, vllm_config):
        """é¢„çƒ­æ¨¡å‹æƒé‡åˆ°CPU - æ¥æ”¶å¤–éƒ¨æä¾›çš„å®Œæ•´vllm_config"""
        global _global_cpu_model, _global_cpu_weights, _global_total_bytes, _global_preheated_path
        
        model_path = vllm_config.model_config.model
        print(f"=== [{id(self)}] é¢„çƒ­æ¨¡å‹: {model_path} ===")
        torch.cuda.empty_cache()
        
        t0 = time.perf_counter()
        
        # åˆ›å»ºvllm_configçš„ç‹¬ç«‹å‰¯æœ¬ï¼Œé¿å…æ±¡æŸ“åŸå§‹é…ç½®
        import copy
        
        print("åˆ›å»ºvllm_configå‰¯æœ¬å¹¶åˆå§‹åŒ–CPUæ¨¡å‹...")
        preheat_vllm_config = copy.deepcopy(vllm_config)
        print(f"é…ç½®dtype: {preheat_vllm_config.model_config.dtype}")
        
        # æµ‹è¯•ï¼šä½¿ç”¨AutoModelForCausalLMæ¥å¯¹æ¯”æ€§èƒ½å·®å¼‚
        USE_TRANSFORMERS_MODEL = os.environ.get("USE_TRANSFORMERS_MODEL", "0") == "1"
        
        if USE_TRANSFORMERS_MODEL:
            print("ğŸ”¬ å®éªŒ: ä½¿ç”¨transformers AutoModelForCausalLM")
            cpu_model = AutoModelForCausalLM.from_pretrained(
                model_path,
                dtype=preheat_vllm_config.model_config.dtype,
                device_map="cpu",
                trust_remote_code=True
            )
        else:
            print("ğŸ“Š ä½¿ç”¨vLLM initialize_model")
            with torch.device("cpu"):
                mock_coordinator = MockGroupCoordinator()
                with patch.object(vllm.distributed, 'get_pp_group',
      return_value=mock_coordinator), \
                       patch.object(vllm.distributed, 'get_tp_group',
      return_value=mock_coordinator), \
                       patch.object(parallel_state, 'get_pp_group',
      return_value=mock_coordinator), \
                       patch.object(parallel_state, 'get_tp_group',
      return_value=mock_coordinator), \
                       set_default_torch_dtype(preheat_vllm_config.model_config.dtype):
                    cpu_model = initialize_model(vllm_config=preheat_vllm_config)
        
        # æƒé‡æå–
        cpu_weights = [p.detach() for p in cpu_model.parameters()]
        total_bytes = sum(w.numel() * w.element_size() for w in cpu_weights)
        
        # è°ƒè¯•ï¼šæ£€æŸ¥æƒé‡æ•°æ®ç±»å‹å’Œå‚æ•°ç»Ÿè®¡
        if cpu_weights:
            print(f"é¢„çƒ­æƒé‡ç±»å‹: {cpu_weights[0].dtype}")
            print(f"æƒé‡è®¾å¤‡: {cpu_weights[0].device}")
            print(f"æ¨¡å‹ç±»å‹: {type(cpu_model).__name__}")
            print(f"å‚æ•°æ•°é‡: {len(cpu_weights)}")
            
            # è®¡ç®—å®é™…å‚æ•°é‡
            total_params = sum(p.numel() for p in cpu_weights)
            print(f"æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e9:.1f}B)")
        
        # å­˜å‚¨åˆ°å…¨å±€çŠ¶æ€ (ä¿ç•™CPUæ¨¡å‹ç”¨äºå°±åœ°è½¬æ¢)
        _global_cpu_model = cpu_model
        _global_cpu_weights = cpu_weights
        _global_total_bytes = total_bytes
        _global_preheated_path = model_path
        
        # ä¸åˆ é™¤cpu_modelï¼Œä¿ç•™ç”¨äºå°±åœ°è½¬æ¢
        gc.collect()
        
        t1 = time.perf_counter()
        print(f"é¢„çƒ­å®Œæˆ: {t1-t0:.3f}s, {human_size(total_bytes)}, {len(cpu_weights)}ä¸ªæƒé‡")
        return {'time': t1-t0, 'bytes': total_bytes, 'count': len(cpu_weights)}
    
    def download_model(self, model_config: ModelConfig) -> None:
        return

    @torch.no_grad()  
    def load_weights(self, model: nn.Module, model_config: ModelConfig) -> None:
        """å…¼å®¹æ¥å£ - ç°åœ¨ä½¿ç”¨å°±åœ°è½¬æ¢æ¨¡å¼ï¼Œæ­¤æ–¹æ³•ä¸ºç©ºå®ç°"""
        print("âš ï¸  load_weightsè¢«è°ƒç”¨ï¼Œä½†ç°åœ¨ä½¿ç”¨å°±åœ°è½¬æ¢æ¨¡å¼")
        return

    def load_model(self, vllm_config: VllmConfig, model_config: ModelConfig) -> nn.Module:
        global _global_cpu_model, _global_cpu_weights, _global_preheated_path
        
        model_path = model_config.model
        
        # è‡ªåŠ¨é¢„çƒ­ï¼ˆå¦‚æœæœªé¢„çƒ­æˆ–è·¯å¾„ä¸åŒ¹é…ï¼‰
        if not _global_cpu_weights:
            print("âš ï¸  æ¨¡å‹æœªé¢„çƒ­ï¼Œè‡ªåŠ¨è¿›è¡Œé¢„çƒ­...")
            self.preheat(vllm_config)
        elif _global_preheated_path != model_path:
            print(f"âš ï¸  è·¯å¾„ä¸åŒ¹é… ({_global_preheated_path} vs {model_path})ï¼Œé‡æ–°é¢„çƒ­...")
            self.preheat(vllm_config)
        else:
            print("âœ… ä½¿ç”¨å·²é¢„çƒ­çš„æƒé‡")
        
        # å°±åœ°è½¬æ¢CPUæ¨¡å‹åˆ°GPU (å¤åˆ¶inference testçš„é«˜æ€§èƒ½æ¨¡å¼)
        print("=== å°±åœ°è½¬æ¢CPUæ¨¡å‹åˆ°GPU ===")
        self._transfer_cpu_model_to_gpu(_global_cpu_model)
        
        # è·å–è½¬æ¢åçš„æ¨¡å‹å¹¶æ¸…ç†å…¨å±€çŠ¶æ€
        model = _global_cpu_model
        # _global_cpu_model = None
        # _global_cpu_weights = None
        # _global_total_bytes = 0
        # _global_preheated_path = None
        # gc.collect()
        
        return model

    def _transfer_cpu_model_to_gpu(self, cpu_model):
        """å°±åœ°å°†CPUæ¨¡å‹è½¬æ¢åˆ°GPU - å®Œå…¨å¤åˆ¶inference testçš„é«˜æ€§èƒ½é€»è¾‘"""
        global _global_cpu_weights, _global_total_bytes
        
        if not _global_cpu_weights:
            raise ValueError("CPUæƒé‡æœªå°±ç»ª")
            
        # è·å–æ¨¡å‹å‚æ•° (ä¸inference testç›¸åŒ)
        params = list(cpu_model.parameters())
        
        print(f"=== GPUä¼ è¾“ (æ‰¹å¤§å°={BATCH_SIZE}) ===")
        print(f"æƒé‡æ•°é‡: {len(_global_cpu_weights)}")
        
        # æ£€æŸ¥å†…å­˜è¿ç»­æ€§
        # if _global_cpu_weights:
        #     first_weight = _global_cpu_weights[0]
        #     print(f"é¦–ä¸ªæƒé‡è¿ç»­æ€§: {first_weight.is_contiguous()}")
        #     print(f"é¦–ä¸ªæƒé‡stride: {first_weight.stride()}")
            
        # æ‰¹é‡ä¼ è¾“ (å®Œå…¨å¤åˆ¶inference testé€»è¾‘)
        t0 = time.perf_counter()
        with torch.no_grad():
            for i in range(0, len(_global_cpu_weights), BATCH_SIZE):
                batch = _global_cpu_weights[i:i+BATCH_SIZE]
                for j, w in enumerate(batch):
                    g = w.to(TARGET_DEVICE, non_blocking=True)
                    params[i + j].data = g  # å°±åœ°ä¿®æ”¹åŒä¸€æ¨¡å‹çš„å‚æ•°
                torch.cuda.synchronize()
                
        t1 = time.perf_counter()
        bandwidth = gbps(_global_total_bytes, t1 - t0)
        print(f"ä¼ è¾“: {t1-t0:.3f}s, {bandwidth:.2f}GB/s ({bandwidth/12.55*100:.1f}%æ•ˆç‡)")

# æµ‹è¯•å‡½æ•°
def _test(model_path: str):
    from vllm import LLM
    from vllm.config import LoadConfig
    
    print("=== FastMemmapLoader æµ‹è¯• ===")
    llm = LLM(model=model_path, load_format="memmap", enforce_eager=True, dtype=torch.float16)
    print("llm initialized")
    
    # ç‹¬ç«‹é¢„çƒ­ - éœ€è¦å¤–éƒ¨æä¾›vllm_config
    loader = FastMemmapLoader(LoadConfig(load_format="memmap"))
    loader.preheat(llm.llm_engine.vllm_config)
    
    # vLLM åŠ è½½ï¼ˆä¼šä½¿ç”¨é¢„çƒ­çš„æƒé‡ï¼‰
    t0 = time.time()
    print(f"vLLM åˆå§‹åŒ–: {time.time()-t0:.3f}s")
    
    # æ¿€æ´»
    print("vLLM æ¿€æ´»")
    llm.llm_engine.v1_vllm_engine_activate()
    
    # æ¨ç†æµ‹è¯•
    # out = llm.generate(["Hello"])
    # print(f"è¾“å‡º: {out[0].outputs[0].text}")

if __name__ == "__main__":
    # å¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡æµ‹è¯•ä¸åŒé…ç½®
    # MEMMAP_BATCH_SIZE=20 python import_weights.py
    _test("Qwen/Qwen3-14B")