#!/usr/bin/env python3
"""
vLLMæ¨¡å‹åŠ è½½æ€§èƒ½æµ‹è¯• - åŸºäºinference testä½†ä½¿ç”¨vLLMæ¨¡å‹
å¯¹æ¯”transformersæ¨¡å‹å’ŒvLLMæ¨¡å‹çš„ä¼ è¾“æ€§èƒ½å·®å¼‚
"""

import torch
import time
import gc
import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer
from vllm.config import VllmConfig, ModelConfig, CacheConfig, ParallelConfig, SchedulerConfig, DeviceConfig, DecodingConfig, CompilationConfig
from vllm.model_executor.model_loader.utils import initialize_model, set_default_torch_dtype
from unittest.mock import patch
import vllm.distributed
import vllm.distributed.parallel_state as parallel_state

def human_size(nbytes):
    gb = 1024**3
    return f"{nbytes/gb:.2f} GB"

def gbps(nbytes, secs):
    return (nbytes / (1024**3)) / max(secs, 1e-9)

class MockGroupCoordinator:
    @property
    def is_first_rank(self):
        return True
    @property
    def is_last_rank(self):
        return True
    @property
    def rank_in_group(self):
        return 0
    @property
    def world_size(self):
        return 1

class VLLMLoadingTester:
    def __init__(self, model_path, device="cuda:0", batch_size=20, use_transformers=False):
        self.model_path = model_path
        self.device = device
        self.batch_size = batch_size
        self.use_transformers = use_transformers
        
    def create_minimal_vllm_config(self):
        """åˆ›å»ºæœ€å°åŒ–çš„vLLMé…ç½®"""
        model_config = ModelConfig(
            model=self.model_path,
            dtype=torch.float16,
            seed=0,
            max_model_len=4096,  # æœ€å°é•¿åº¦
            skip_tokenizer_init=True
        )
        
        # åˆ›å»ºå¿…éœ€çš„é…ç½®å¯¹è±¡
        cache_config = CacheConfig(
            block_size=16,
            gpu_memory_utilization=0.9,
            swap_space=4,
            cache_dtype="auto"
        )
        
        parallel_config = ParallelConfig(
            pipeline_parallel_size=1,
            tensor_parallel_size=1,
            worker_use_ray=False,
            max_parallel_loading_workers=None,
            disable_custom_all_reduce=False,
            tokenizer_pool_size=0,
            tokenizer_pool_type="ray",
            tokenizer_pool_extra_config={},
            ray_workers_use_nsight=False,
            placement_group=None
        )
        
        scheduler_config = SchedulerConfig(
            max_num_batched_tokens=None,
            max_num_seqs=256,
            max_model_len=model_config.max_model_len,
            use_v2_block_manager=False,
            num_lookahead_slots=0,
            delay_factor=0.0,
            enable_chunked_prefill=False,
            max_num_on_the_fly=1,
            policy="fcfs",
        )
        
        device_config = DeviceConfig(device="cuda")
        
        decoding_config = DecodingConfig()
        
        compilation_config = CompilationConfig()
        
        return VllmConfig(
            model_config=model_config,
            cache_config=cache_config,
            parallel_config=parallel_config,
            scheduler_config=scheduler_config,
            device_config=device_config,
            lora_config=None,
            speculative_config=None,
            decoding_config=decoding_config,
            observability_config=None,
            prompt_adapter_config=None,
            quant_config=None,
            compilation_config=compilation_config,
        )
    
    def load_model_comparison(self):
        print(f"\n=== æ¨¡å‹åŠ è½½å¯¹æ¯”æµ‹è¯•: {self.model_path} ===")
        print(f"ä½¿ç”¨æ¨¡å‹ç±»å‹: {'transformers' if self.use_transformers else 'vLLM'}")

        if self.use_transformers:
            # åŸå§‹transformersæ–¹å¼
            print("\n--- transformers AutoModelForCausalLM ---")
            t0 = time.perf_counter()
            cpu_model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                dtype=torch.float16,
                device_map="cpu",
                trust_remote_code=True
            )
            t1 = time.perf_counter()
        else:
            # vLLMæ–¹å¼
            print("\n--- vLLM initialize_model ---")
            vllm_config = self.create_minimal_vllm_config()
            
            t0 = time.perf_counter()
            with torch.device("cpu"):
                mock_coordinator = MockGroupCoordinator()
                with patch.object(vllm.distributed, 'get_pp_group',
                                return_value=mock_coordinator), \
                     patch.object(vllm.distributed, 'get_tp_group',
                                return_value=mock_coordinator), \
                     patch.object(parallel_state, 'get_pp_group',
                                return_value=mock_coordinator), \
                     patch.object(parallel_state, 'get_tp_group',
                                return_value=mock_coordinator), \
                     set_default_torch_dtype(torch.float16):
                    cpu_model = initialize_model(vllm_config=vllm_config)
            t1 = time.perf_counter()

        cpu_load_time = t1 - t0

        # æƒé‡æå–å’Œåˆ†æ
        print("\næ­¥éª¤2: æƒé‡æå–å’Œåˆ†æ...")
        t2 = time.perf_counter()
        params = list(cpu_model.parameters())
        cpu_weights = [p.detach() for p in params]
        total_bytes = sum(w.numel() * w.element_size() for w in cpu_weights)
        t3 = time.perf_counter()
        extract_time = t3 - t2

        print(f"CPUåŠ è½½: {cpu_load_time:.3f}s")
        print(f"æƒé‡æå–: {extract_time:.3f}s")
        print(f"æ¨¡å‹å¤§å°: {human_size(total_bytes)}")
        print(f"æ¨¡å‹ç±»å‹: {type(cpu_model).__name__}")
        print(f"æƒé‡æ•°é‡: {len(cpu_weights)}")
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in cpu_weights)
        print(f"æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e9:.1f}B)")

        # è¯¦ç»†æƒé‡åˆ†å¸ƒåˆ†æ
        print(f"\næƒé‡åˆ†å¸ƒåˆ†æ:")
        
        # æ›´ç»†åŒ–çš„å¤§å°åˆ†ç±»
        tiny_weights = sum(1 for w in cpu_weights if w.numel() < 1000)         # <1K
        small_weights = sum(1 for w in cpu_weights if 1000 <= w.numel() < 100000)    # 1K-100K
        medium_weights = sum(1 for w in cpu_weights if 100000 <= w.numel() < 10000000)  # 100K-10M
        large_weights = sum(1 for w in cpu_weights if 10000000 <= w.numel() < 100000000)  # 10M-100M
        huge_weights = sum(1 for w in cpu_weights if w.numel() >= 100000000)   # >100M
        
        print(f"å¾®å°æƒé‡(<1K): {tiny_weights}")
        print(f"å°æƒé‡(1K-100K): {small_weights}")  
        print(f"ä¸­æƒé‡(100K-10M): {medium_weights}")
        print(f"å¤§æƒé‡(10M-100M): {large_weights}")
        print(f"å·¨å¤§æƒé‡(>100M): {huge_weights}")
        
        # æœ€å¤§æƒé‡åˆ†æ
        max_weight_size = max(w.numel() for w in cpu_weights)
        max_weight_mb = max_weight_size * 2 / 1024 / 1024  # float16 = 2 bytes
        print(f"æœ€å¤§æƒé‡: {max_weight_size:,} å…ƒç´  ({max_weight_mb:.1f}MB)")
        
        # Top 10å¤§æƒé‡åˆ†æ
        weight_sizes = sorted([w.numel() for w in cpu_weights], reverse=True)[:10]
        print(f"Top 10æƒé‡å¤§å°:")
        for i, size in enumerate(weight_sizes):
            mb = size * 2 / 1024 / 1024
            print(f"  {i+1}. {size:,} å…ƒç´  ({mb:.1f}MB)")
        
        # æŒ‰å¤§å°ç»Ÿè®¡æ€»å†…å­˜å æ¯”
        total_elements = sum(w.numel() for w in cpu_weights)
        tiny_elements = sum(w.numel() for w in cpu_weights if w.numel() < 1000)
        small_elements = sum(w.numel() for w in cpu_weights if 1000 <= w.numel() < 100000)
        medium_elements = sum(w.numel() for w in cpu_weights if 100000 <= w.numel() < 10000000)
        large_elements = sum(w.numel() for w in cpu_weights if 10000000 <= w.numel() < 100000000)
        huge_elements = sum(w.numel() for w in cpu_weights if w.numel() >= 100000000)
        
        print(f"\nå†…å­˜å æ¯”:")
        print(f"å¾®å°æƒé‡: {tiny_elements/total_elements*100:.1f}%")
        print(f"å°æƒé‡: {small_elements/total_elements*100:.1f}%")
        print(f"ä¸­æƒé‡: {medium_elements/total_elements*100:.1f}%")  
        print(f"å¤§æƒé‡: {large_elements/total_elements*100:.1f}%")
        print(f"å·¨å¤§æƒé‡: {huge_elements/total_elements*100:.1f}%")

        # ç”¨æˆ·ç¡®è®¤
        print("\nâ¸ï¸  å‡†å¤‡å¼€å§‹GPUä¼ è¾“")
        input("æŒ‰Enterç»§ç»­...")

        # GPUä¼ è¾“æµ‹è¯•
        print(f"\næ­¥éª¤3: æ‰¹é‡ä¼ è¾“æµ‹è¯• (æ‰¹å¤§å°={self.batch_size})...")
        t4 = time.perf_counter()
        with torch.no_grad():
            for i in range(0, len(cpu_weights), self.batch_size):
                batch = cpu_weights[i:i+self.batch_size]
                for j, w in enumerate(batch):
                    g = w.to(self.device, non_blocking=True)
                    params[i + j].data = g
                torch.cuda.synchronize()
        t5 = time.perf_counter()
        transfer_time = t5 - t4
        bandwidth = gbps(total_bytes, transfer_time)
        
        print(f"ä¼ è¾“æ—¶é—´: {transfer_time:.3f}s")
        print(f"ä¼ è¾“å¸¦å®½: {bandwidth:.2f} GB/s")
        print(f"ç¡¬ä»¶æ•ˆç‡: {bandwidth/12.55*100:.1f}%")

        # æ¸…ç†
        del cpu_model, cpu_weights, params
        gc.collect()
        
        return {
            'model_type': 'transformers' if self.use_transformers else 'vLLM',
            'cpu_load_time': cpu_load_time,
            'extract_time': extract_time,
            'transfer_time': transfer_time,
            'bandwidth': bandwidth,
            'total_bytes': total_bytes,
            'weight_count': len(cpu_weights),
            'tiny_weights': tiny_weights,
            'small_weights': small_weights,
            'medium_weights': medium_weights,
            'large_weights': large_weights,
            'huge_weights': huge_weights,
            'max_weight_size': max_weight_size,
            'top10_weights': weight_sizes
        }

def main():
    parser = argparse.ArgumentParser(description="vLLM vs transformers åŠ è½½æ€§èƒ½å¯¹æ¯”")
    parser.add_argument("--model", default="Qwen/Qwen3-14B", help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--device", default="cuda:0", help="GPUè®¾å¤‡")
    parser.add_argument("--batch-size", type=int, default=20, help="æ‰¹é‡å¤§å°")
    parser.add_argument("--both", action="store_true", help="åŒæ—¶æµ‹è¯•ä¸¤ç§æ–¹å¼")
    parser.add_argument("--transformers", action="store_true", help="ä½¿ç”¨transformersæ¨¡å‹")
    
    args = parser.parse_args()
    
    if args.both:
        print("=== åŒæ—¶å¯¹æ¯”ä¸¤ç§æ¨¡å¼ ===")
        
        # æµ‹è¯•transformers
        print("\nğŸ”¬ æµ‹è¯•1: transformersæ¨¡å‹")
        tester1 = VLLMLoadingTester(args.model, args.device, args.batch_size, use_transformers=True)
        result1 = tester1.load_model_comparison()
        
        # æ¸…ç†åæµ‹è¯•vLLM
        torch.cuda.empty_cache()
        gc.collect()
        
        print("\nğŸ”¬ æµ‹è¯•2: vLLMæ¨¡å‹") 
        tester2 = VLLMLoadingTester(args.model, args.device, args.batch_size, use_transformers=False)
        result2 = tester2.load_model_comparison()
        
        # å¯¹æ¯”ç»“æœ
        print(f"\n{'='*60}")
        print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“")
        print(f"{'='*60}")
        print(f"{'æŒ‡æ ‡':<20} {'transformers':<15} {'vLLM':<15} {'å·®å¼‚'}")
        print(f"{'-'*60}")
        print(f"{'æƒé‡æ•°é‡':<20} {result1['weight_count']:<15} {result2['weight_count']:<15} {result2['weight_count']/result1['weight_count']:.2f}x")
        print(f"{'ä¼ è¾“å¸¦å®½(GB/s)':<20} {result1['bandwidth']:<15.2f} {result2['bandwidth']:<15.2f} {result2['bandwidth']/result1['bandwidth']:.2f}x")
        print(f"{'ä¼ è¾“æ—¶é—´(s)':<20} {result1['transfer_time']:<15.3f} {result2['transfer_time']:<15.3f} {result2['transfer_time']/result1['transfer_time']:.2f}x")
        print(f"{'ç¡¬ä»¶æ•ˆç‡(%)':<20} {result1['bandwidth']/12.55*100:<15.1f} {result2['bandwidth']/12.55*100:<15.1f}")
        
        print(f"\næƒé‡åˆ†å¸ƒå¯¹æ¯”:")
        print(f"{'ç±»å‹':<15} {'transformers':<15} {'vLLM':<15} {'å·®å¼‚'}")
        print(f"{'-'*55}")
        print(f"{'å¾®å°(<1K)':<15} {result1['tiny_weights']:<15} {result2['tiny_weights']:<15} {result2['tiny_weights']-result1['tiny_weights']:+d}")
        print(f"{'å°(1K-100K)':<15} {result1['small_weights']:<15} {result2['small_weights']:<15} {result2['small_weights']-result1['small_weights']:+d}")
        print(f"{'ä¸­(100K-10M)':<15} {result1['medium_weights']:<15} {result2['medium_weights']:<15} {result2['medium_weights']-result1['medium_weights']:+d}")
        print(f"{'å¤§(10M-100M)':<15} {result1['large_weights']:<15} {result2['large_weights']:<15} {result2['large_weights']-result1['large_weights']:+d}")
        print(f"{'å·¨å¤§(>100M)':<15} {result1['huge_weights']:<15} {result2['huge_weights']:<15} {result2['huge_weights']-result1['huge_weights']:+d}")
        
        print(f"\næœ€å¤§æƒé‡å¯¹æ¯”:")
        t_max_mb = result1['max_weight_size'] * 2 / 1024 / 1024
        v_max_mb = result2['max_weight_size'] * 2 / 1024 / 1024
        print(f"transformersæœ€å¤§æƒé‡: {result1['max_weight_size']:,} ({t_max_mb:.1f}MB)")
        print(f"vLLMæœ€å¤§æƒé‡: {result2['max_weight_size']:,} ({v_max_mb:.1f}MB)")
        print(f"å·®å¼‚: {result2['max_weight_size']/result1['max_weight_size']:.2f}x")
        
    else:
        # å•ç‹¬æµ‹è¯•
        use_transformers = args.transformers
        tester = VLLMLoadingTester(args.model, args.device, args.batch_size, use_transformers)
        result = tester.load_model_comparison()
        print(f"\nâœ… æµ‹è¯•å®Œæˆ - {result['model_type']}æ¨¡å‹")

if __name__ == "__main__":
    main()