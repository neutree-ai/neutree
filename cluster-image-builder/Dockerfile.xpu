# Intel XPU Dockerfile (vLLM + Neutree custom), refer to https://github.com/vllm-project/vllm/blob/v0.8.5/docker/Dockerfile.xpu
ARG BASE_IMAGE
ARG VLLM_BRANCH="v0.8.5"
ARG VLLM_REPO="https://github.com/vllm-project/vllm.git"

FROM ${BASE_IMAGE} AS base

# Clean up Intel graphics repo (per vllm Dockerfile)
RUN rm -f /etc/apt/sources.list.d/intel-graphics.list || true

RUN sed -Ei 's@http://(security|archive).ubuntu.com/ubuntu@https://mirrors.aliyun.com/ubuntu@g' \
  /etc/apt/sources.list

# Install system dependencies
RUN apt-get update -y && \
  apt-get install -y --no-install-recommends --fix-missing \
  curl ffmpeg git libsndfile1 libsm6 libxext6 libgl1 lsb-release numactl \
  python-is-python3 python3 python3-dev python3-pip wget sqlite3 libsqlite3-dev libfmt-dev libmsgpack-dev libsuitesparse-dev apt-transport-https ca-certificates rsync && \
  rm -rf /var/lib/apt/lists/*

# Install xpu-smi
RUN set -eux; \
  apt-get update -y && \
  apt-get install -y --no-install-recommends wget gpg ca-certificates; \
  \
  wget -qO- https://repositories.intel.com/gpu/intel-graphics.key | \
  gpg --dearmor -o /usr/share/keyrings/intel-gpu.gpg; \
  \
  echo "deb [arch=amd64 signed-by=/usr/share/keyrings/intel-gpu.gpg] \
  https://repositories.intel.com/gpu/ubuntu jammy client" \
  > /etc/apt/sources.list.d/intel-gpu.list; \
  \
  apt-get update -y && \
  apt-get install -y --no-install-recommends xpu-smi; \
  \
  apt-get clean && rm -rf /var/lib/apt/lists/*

WORKDIR /workspace/vllm

# Clone and build vllm
ARG VLLM_REPO
ARG VLLM_BRANCH
RUN git clone ${VLLM_REPO} . && git checkout ${VLLM_BRANCH}
ENV VLLM_TARGET_DEVICE=xpu

# Install Python dependencies for vllm
RUN --mount=type=cache,target=/root/.cache/pip \
  python3 -m pip install --upgrade pip && \
  pip install --no-cache-dir -r requirements/xpu.txt

# Install ipex (intel-extension-for-pytorch) per vllm Dockerfile.xpu
RUN --mount=type=cache,target=/root/.cache/pip \
  pip install intel-extension-for-pytorch==2.6.10+xpu --extra-index-url=https://pytorch-extension.intel.com/release-whl/stable/xpu/us/

# RAY VERSION FIX: Check Ray version before vLLM installation
RUN echo "=== Ray version before vLLM install ===" && \
  python3 -c "import ray; print(f'Ray version: {ray.__version__}')" 2>/dev/null || echo "Ray not yet installed"

# Install vLLM (this may upgrade Ray version)
RUN --mount=type=cache,target=/root/.cache/pip \
  python3 setup.py install

# RAY VERSION FIX: Check Ray version after vLLM installation and force correct version
RUN echo "=== Ray version after vLLM install ===" && \
  python3 -c "import ray; print(f'Ray version: {ray.__version__}')" 2>/dev/null && \
  echo "=== Forcing Ray version to 2.43.0 ===" && \
  pip install "ray[serve]==2.43.0" --force-reinstall && \
  echo "=== Final Ray version ===" && \
  python3 -c "import ray; print(f'Final Ray version: {ray.__version__}')"

# Copy requirements
COPY requirements/xpu.txt /workspace/vllm/requirements/xpu.txt
COPY requirements/common.txt /workspace/vllm/requirements/common.txt

# Install huggingface-hub cli and common requirements
RUN python3 -m pip install --upgrade huggingface-hub[cli]
RUN pip install --no-cache-dir -r requirements/common.txt

# RAY VERSION FIX: Final verification that Ray version is correct
RUN python3 -c "import ray; assert ray.__version__.startswith('2.43.'), f'❌ Expected Ray 2.43.x, got {ray.__version__}'; print('✅ Ray version verified: ' + ray.__version__)"

FROM base

ENV HOME=/home/ray
RUN mkdir -p ${HOME}
WORKDIR ${HOME}

# Copy Python files
COPY *.py ./
COPY serve ./serve
COPY accelerator ./accelerator

RUN chmod +x start.py

EXPOSE 8265 6379 8000

CMD ["/bin/bash"]
