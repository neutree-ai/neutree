{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "title": "vLLM v0.11.2 Engine Configuration",
  "description": "Configuration schema for vLLM v0.11.2 engine parameters",
  "properties": {
    "model": {
      "type": "string",
      "description": "Name or path of the Hugging Face model to use"
    },
    "runner": {
      "type": "string",
      "enum": ["auto", "draft", "generate", "pooling"],
      "default": "auto",
      "description": "The type of model runner to use"
    },
    "convert": {
      "type": "string",
      "enum": ["auto", "classify", "embed", "none", "reward"],
      "default": "auto",
      "description": "Convert the model using adapters"
    },
    "tokenizer": {
      "type": "string",
      "description": "Name or path of the Hugging Face tokenizer to use"
    },
    "tokenizer_mode": {
      "type": "string",
      "enum": ["auto", "slow", "mistral", "custom"],
      "default": "auto",
      "description": "The tokenizer mode to use"
    },
    "trust_remote_code": {
      "type": "boolean",
      "default": false,
      "description": "Trust remote code from Hugging Face"
    },
    "dtype": {
      "type": "string",
      "enum": ["auto", "half", "float16", "bfloat16", "float", "float32"],
      "default": "auto",
      "description": "Data type for model weights and activations"
    },
    "seed": {
      "type": "integer",
      "minimum": 0,
      "description": "Random seed for reproducibility"
    },
    "hf_config_path": {
      "type": "string",
      "description": "Name or path of the Hugging Face config to use"
    },
    "allowed_local_media_path": {
      "type": "string",
      "default": "",
      "description": "Allowed local media path for security"
    },
    "allowed_media_domains": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Allowed media domains for multi-modal inputs"
    },
    "revision": {
      "type": "string",
      "description": "The specific model version to use"
    },
    "code_revision": {
      "type": "string",
      "description": "The specific revision to use for the model code"
    },
    "tokenizer_revision": {
      "type": "string",
      "description": "Revision of the tokenizer to use"
    },
    "max_model_len": {
      "type": ["integer", "string"],
      "description": "Model context length. Supports human-readable format like '1k', '2M'"
    },
    "quantization": {
      "type": "string",
      "description": "Method used to quantize the weights"
    },
    "enforce_eager": {
      "type": "boolean",
      "default": false,
      "description": "Always use eager-mode PyTorch"
    },
    "max_logprobs": {
      "type": "integer",
      "default": 20,
      "description": "Maximum number of log probabilities to return"
    },
    "logprobs_mode": {
      "type": "string",
      "enum": ["processed_logits", "processed_logprobs", "raw_logits", "raw_logprobs"],
      "default": "raw_logprobs",
      "description": "Indicates the content returned in logprobs"
    },
    "disable_sliding_window": {
      "type": "boolean",
      "default": false,
      "description": "Disable sliding window attention"
    },
    "disable_cascade_attn": {
      "type": "boolean",
      "default": false,
      "description": "Disable cascade attention for V1"
    },
    "skip_tokenizer_init": {
      "type": "boolean",
      "default": false,
      "description": "Skip initialization of tokenizer and detokenizer"
    },
    "enable_prompt_embeds": {
      "type": "boolean",
      "default": false,
      "description": "Enable passing text embeddings via prompt_embeds"
    },
    "served_model_name": {
      "type": ["string", "array"],
      "items": {
        "type": "string"
      },
      "description": "The model name(s) used in the API"
    },
    "config_format": {
      "type": "string",
      "enum": ["auto", "hf", "mistral"],
      "default": "auto",
      "description": "The format of the model config to load"
    },
    "hf_token": {
      "type": ["string", "boolean"],
      "description": "Hugging Face token for authentication"
    },
    "hf_overrides": {
      "type": "object",
      "default": {},
      "description": "Arguments to be forwarded to HuggingFace config"
    },
    "pooler_config": {
      "type": ["string", "object"],
      "description": "Pooler config for output pooling behavior"
    },
    "override_pooler_config": {
      "type": ["string", "object"],
      "description": "[DEPRECATED] Use pooler_config instead"
    },
    "logits_processor_pattern": {
      "type": "string",
      "description": "Regex pattern for valid logits processor names"
    },
    "generation_config": {
      "type": "string",
      "default": "auto",
      "description": "Path to generation config"
    },
    "override_generation_config": {
      "type": "object",
      "default": {},
      "description": "Override generation config parameters"
    },
    "enable_sleep_mode": {
      "type": "boolean",
      "default": false,
      "description": "Enable sleep mode for the engine"
    },
    "model_impl": {
      "type": "string",
      "enum": ["auto", "terratorch", "transformers", "vllm"],
      "default": "auto",
      "description": "Which implementation of the model to use"
    },
    "override_attention_dtype": {
      "type": "string",
      "description": "Override dtype for attention"
    },
    "logits_processors": {
      "type": ["string", "array"],
      "items": {
        "type": "string"
      },
      "description": "Logits processors class names"
    },
    "io_processor_plugin": {
      "type": "string",
      "description": "IOProcessor plugin name to load at startup"
    },
    "load_format": {
      "type": "string",
      "default": "auto",
      "description": "The format of model weights to load"
    },
    "download_dir": {
      "type": "string",
      "description": "Directory to download and load weights"
    },
    "safetensors_load_strategy": {
      "type": "string",
      "enum": ["lazy", "eager", "torchao"],
      "default": "lazy",
      "description": "Loading strategy for safetensors weights"
    },
    "model_loader_extra_config": {
      "type": "object",
      "default": {},
      "description": "Extra config for model loader"
    },
    "ignore_patterns": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "default": ["original/**/*"],
      "description": "Patterns to ignore when loading model"
    },
    "use_tqdm_on_load": {
      "type": "boolean",
      "default": true,
      "description": "Enable tqdm progress bar when loading"
    },
    "pt_load_map_location": {
      "type": ["string", "object"],
      "default": "cpu",
      "description": "Map location for loading pytorch checkpoint"
    },
    "reasoning_parser": {
      "type": "string",
      "default": "",
      "description": "Reasoning parser to use"
    },
    "reasoning_parser_plugin": {
      "type": "string",
      "default": "",
      "description": "Path to reasoning parser plugin"
    },
    "guided_decoding_backend": {
      "type": "string",
      "description": "[DEPRECATED] Guided decoding backend"
    },
    "guided_decoding_disable_fallback": {
      "type": "boolean",
      "description": "[DEPRECATED] Disable fallback in guided decoding"
    },
    "guided_decoding_disable_any_whitespace": {
      "type": "boolean",
      "description": "[DEPRECATED] Disable any whitespace"
    },
    "guided_decoding_disable_additional_properties": {
      "type": "boolean",
      "description": "[DEPRECATED] Disable additional properties"
    },
    "distributed_executor_backend": {
      "type": "string",
      "enum": ["external_launcher", "mp", "ray", "uni"],
      "description": "Backend for distributed model workers"
    },
    "pipeline_parallel_size": {
      "type": "integer",
      "minimum": 1,
      "default": 1,
      "description": "Number of pipeline parallel groups"
    },
    "master_addr": {
      "type": "string",
      "default": "127.0.0.1",
      "description": "Distributed master address"
    },
    "master_port": {
      "type": "integer",
      "default": 29501,
      "description": "Distributed master port"
    },
    "nnodes": {
      "type": "integer",
      "minimum": 1,
      "default": 1,
      "description": "Number of nodes for multi-node inference"
    },
    "node_rank": {
      "type": "integer",
      "minimum": 0,
      "default": 0,
      "description": "Distributed node rank"
    },
    "tensor_parallel_size": {
      "type": "integer",
      "minimum": 1,
      "default": 1,
      "description": "Number of tensor parallel groups"
    },
    "decode_context_parallel_size": {
      "type": "integer",
      "minimum": 1,
      "default": 1,
      "description": "Number of decode context parallel groups"
    },
    "dcp_kv_cache_interleave_size": {
      "type": "integer",
      "description": "KV cache interleave size for decode context parallelism"
    },
    "data_parallel_size": {
      "type": "integer",
      "minimum": 1,
      "default": 1,
      "description": "Number of data parallel replicas"
    },
    "data_parallel_rank": {
      "type": "integer",
      "minimum": 0,
      "description": "Data parallel rank of this instance"
    },
    "data_parallel_start_rank": {
      "type": "integer",
      "minimum": 0,
      "description": "Starting data parallel rank for secondary nodes"
    },
    "data_parallel_size_local": {
      "type": "integer",
      "minimum": 1,
      "description": "Number of data parallel replicas on this node"
    },
    "data_parallel_address": {
      "type": "string",
      "description": "Address of data parallel cluster head-node"
    },
    "data_parallel_rpc_port": {
      "type": "integer",
      "description": "Port for data parallel RPC communication"
    },
    "data_parallel_backend": {
      "type": "string",
      "enum": ["mp", "ray"],
      "default": "mp",
      "description": "Backend for data parallel"
    },
    "data_parallel_hybrid_lb": {
      "type": "boolean",
      "default": false,
      "description": "Use hybrid DP load balancing mode"
    },
    "data_parallel_external_lb": {
      "type": "boolean",
      "default": false,
      "description": "Use external DP load balancing mode"
    },
    "enable_expert_parallel": {
      "type": "boolean",
      "default": false,
      "description": "Enable expert parallelism for MoE models"
    },
    "all2all_backend": {
      "type": "string",
      "enum": ["allgather_reducescatter", "deepep_high_throughput", "deepep_low_latency", "flashinfer_all2allv", "naive", "pplx", "None"],
      "description": "All2All backend for MoE expert parallel"
    },
    "enable_dbo": {
      "type": "boolean",
      "default": false,
      "description": "Enable dual batch overlap"
    },
    "dbo_decode_token_threshold": {
      "type": "integer",
      "default": 32,
      "description": "Token threshold for dual batch overlap decode"
    },
    "dbo_prefill_token_threshold": {
      "type": "integer",
      "default": 512,
      "description": "Token threshold for dual batch overlap prefill"
    },
    "disable_nccl_for_dp_synchronization": {
      "type": "boolean",
      "default": false,
      "description": "Force DP sync to use Gloo instead of NCCL"
    },
    "enable_eplb": {
      "type": "boolean",
      "default": false,
      "description": "Enable expert parallelism load balancing"
    },
    "eplb_config": {
      "type": ["string", "object"],
      "description": "Expert parallelism configuration"
    },
    "expert_placement_strategy": {
      "type": "string",
      "enum": ["linear", "round_robin"],
      "default": "linear",
      "description": "Expert placement strategy for MoE layers"
    },
    "num_redundant_experts": {
      "type": "integer",
      "description": "[DEPRECATED] Number of redundant experts"
    },
    "eplb_window_size": {
      "type": "integer",
      "description": "[DEPRECATED] EPLB window size"
    },
    "eplb_step_interval": {
      "type": "integer",
      "description": "[DEPRECATED] EPLB step interval"
    },
    "eplb_log_balancedness": {
      "type": "boolean",
      "description": "[DEPRECATED] Log EPLB balancedness"
    },
    "max_parallel_loading_workers": {
      "type": "integer",
      "description": "Maximum number of parallel loading workers"
    },
    "ray_workers_use_nsight": {
      "type": "boolean",
      "default": false,
      "description": "Profile Ray workers with nsight"
    },
    "disable_custom_all_reduce": {
      "type": "boolean",
      "default": false,
      "description": "Disable custom all-reduce kernels"
    },
    "worker_cls": {
      "type": "string",
      "default": "auto",
      "description": "Full name of the worker class to use"
    },
    "worker_extension_cls": {
      "type": "string",
      "default": "",
      "description": "Worker extension class name"
    },
    "enable_multimodal_encoder_data_parallel": {
      "type": "boolean",
      "default": false,
      "description": "Enable multimodal encoder data parallel"
    },
    "block_size": {
      "type": "integer",
      "enum": [1, 8, 16, 32, 64, 128, 256],
      "description": "Size of a contiguous cache block"
    },
    "gpu_memory_utilization": {
      "type": "number",
      "minimum": 0,
      "maximum": 1.0,
      "default": 0.9,
      "description": "The fraction of GPU memory to be used"
    },
    "kv_cache_memory_bytes": {
      "type": ["integer", "string"],
      "description": "Size of KV Cache per GPU in bytes"
    },
    "swap_space": {
      "type": "number",
      "minimum": 0,
      "default": 4,
      "description": "CPU swap space size (GiB) per GPU"
    },
    "kv_cache_dtype": {
      "type": "string",
      "enum": ["auto", "bfloat16", "fp8", "fp8_ds_mla", "fp8_e4m3", "fp8_e5m2", "fp8_inc"],
      "default": "auto",
      "description": "Data type for KV cache storage"
    },
    "num_gpu_blocks_override": {
      "type": "integer",
      "description": "Override profiled num_gpu_blocks"
    },
    "enable_prefix_caching": {
      "type": "boolean",
      "description": "Enable prefix caching"
    },
    "prefix_caching_hash_algo": {
      "type": "string",
      "enum": ["sha256", "sha256_cbor"],
      "default": "sha256",
      "description": "Hash algorithm for prefix caching"
    },
    "cpu_offload_gb": {
      "type": "number",
      "minimum": 0,
      "default": 0,
      "description": "The space in GiB to offload to CPU"
    },
    "calculate_kv_scales": {
      "type": "boolean",
      "default": false,
      "description": "Enable dynamic calculation of k_scale and v_scale"
    },
    "kv_sharing_fast_prefill": {
      "type": "boolean",
      "default": false,
      "description": "Enable KV sharing fast prefill optimization"
    },
    "mamba_cache_dtype": {
      "type": "string",
      "enum": ["auto", "float32"],
      "default": "auto",
      "description": "Data type for Mamba cache"
    },
    "mamba_ssm_cache_dtype": {
      "type": "string",
      "enum": ["auto", "float32"],
      "default": "auto",
      "description": "Data type for Mamba SSM state"
    },
    "mamba_block_size": {
      "type": "integer",
      "description": "Block size for mamba cache"
    },
    "kv_offloading_size": {
      "type": "number",
      "description": "Size of KV cache offloading buffer (GiB)"
    },
    "kv_offloading_backend": {
      "type": "string",
      "enum": ["lmcache", "native", "None"],
      "description": "Backend for KV cache offloading"
    },
    "limit_mm_per_prompt": {
      "type": ["string", "object"],
      "default": {},
      "description": "Maximum number of multi-modal items per prompt"
    },
    "enable_mm_embeds": {
      "type": "boolean",
      "default": false,
      "description": "Enable passing multimodal embeddings"
    },
    "media_io_kwargs": {
      "type": ["string", "object"],
      "default": {},
      "description": "Additional args for processing media inputs"
    },
    "mm_processor_kwargs": {
      "type": ["string", "object"],
      "description": "Arguments forwarded to multi-modal processor"
    },
    "mm_processor_cache_gb": {
      "type": "number",
      "default": 4,
      "description": "Size (GiB) of multi-modal processor cache"
    },
    "disable_mm_preprocessor_cache": {
      "type": "boolean",
      "default": false,
      "description": "Disable multi-modal preprocessor cache"
    },
    "mm_processor_cache_type": {
      "type": "string",
      "enum": ["lru", "shm"],
      "default": "lru",
      "description": "Type of cache for multi-modal processor"
    },
    "mm_shm_cache_max_object_size_mb": {
      "type": "integer",
      "default": 128,
      "description": "Max object size (MiB) in shared memory cache"
    },
    "mm_encoder_tp_mode": {
      "type": "string",
      "enum": ["data", "weights"],
      "description": "Multi-modal encoder tensor parallel mode"
    },
    "mm_encoder_attn_backend": {
      "type": "string",
      "description": "Multi-modal encoder attention backend"
    },
    "interleave_mm_strings": {
      "type": "boolean",
      "default": false,
      "description": "Enable fully interleaved multimodal prompts"
    },
    "skip_mm_profiling": {
      "type": "boolean",
      "default": false,
      "description": "Skip multimodal memory profiling"
    },
    "video_pruning_rate": {
      "type": "number",
      "minimum": 0,
      "maximum": 1,
      "description": "Pruning rate for video via Efficient Video Sampling"
    },
    "enable_lora": {
      "type": "boolean",
      "description": "Enable handling of LoRA adapters"
    },
    "max_loras": {
      "type": "integer",
      "minimum": 1,
      "default": 1,
      "description": "Maximum number of LoRA adapters"
    },
    "max_lora_rank": {
      "type": "integer",
      "enum": [1, 8, 16, 32, 64, 128, 256, 320, 512],
      "default": 16,
      "description": "Maximum LoRA rank"
    },
    "lora_extra_vocab_size": {
      "type": "integer",
      "enum": [256, 512],
      "default": 256,
      "description": "[DEPRECATED] Max extra vocabulary size"
    },
    "lora_dtype": {
      "type": "string",
      "default": "auto",
      "description": "Data type for LoRA"
    },
    "max_cpu_loras": {
      "type": "integer",
      "description": "Maximum number of LoRAs to store in CPU memory"
    },
    "fully_sharded_loras": {
      "type": "boolean",
      "default": false,
      "description": "Use fully sharded LoRA layers"
    },
    "default_mm_loras": {
      "type": ["string", "object"],
      "description": "Default LoRA paths for specific modalities"
    },
    "show_hidden_metrics_for_version": {
      "type": "string",
      "description": "Show hidden metrics since specified version"
    },
    "otlp_traces_endpoint": {
      "type": "string",
      "description": "OpenTelemetry traces target URL"
    },
    "collect_detailed_traces": {
      "type": "string",
      "enum": ["all", "model", "worker", "None", "model,worker", "model,all", "worker,model", "worker,all", "all,model", "all,worker"],
      "description": "Collect detailed traces for specified modules"
    },
    "max_num_batched_tokens": {
      "type": ["integer", "string"],
      "description": "Maximum number of batched tokens per iteration"
    },
    "max_num_seqs": {
      "type": "integer",
      "description": "Maximum number of sequences per iteration"
    },
    "max_num_partial_prefills": {
      "type": "integer",
      "default": 1,
      "description": "Max concurrent partial prefills for chunked prefill"
    },
    "max_long_partial_prefills": {
      "type": "integer",
      "default": 1,
      "description": "Max concurrent long prompts for chunked prefill"
    },
    "long_prefill_token_threshold": {
      "type": "integer",
      "default": 0,
      "description": "Token threshold for long prefill"
    },
    "num_lookahead_slots": {
      "type": "integer",
      "default": 0,
      "description": "Slots for speculative decoding"
    },
    "scheduling_policy": {
      "type": "string",
      "enum": ["fcfs", "priority"],
      "default": "fcfs",
      "description": "The scheduling policy to use"
    },
    "enable_chunked_prefill": {
      "type": "boolean",
      "description": "Enable chunked prefill requests"
    },
    "disable_chunked_mm_input": {
      "type": "boolean",
      "default": false,
      "description": "Disable partial scheduling of multimodal items"
    },
    "scheduler_cls": {
      "type": "string",
      "description": "The scheduler class to use"
    },
    "disable_hybrid_kv_cache_manager": {
      "type": "boolean",
      "default": false,
      "description": "Disable hybrid KV cache manager"
    },
    "async_scheduling": {
      "type": "boolean",
      "default": false,
      "description": "Perform async scheduling"
    },
    "stream_interval": {
      "type": "integer",
      "default": 1,
      "description": "Interval for streaming in terms of tokens"
    },
    "cudagraph_capture_sizes": {
      "type": "array",
      "items": {
        "type": "integer"
      },
      "description": "Sizes to capture cudagraph"
    },
    "cuda_graph_sizes": {
      "type": "array",
      "items": {
        "type": "integer"
      },
      "description": "[DEPRECATED] Use cudagraph_capture_sizes"
    },
    "max_cudagraph_capture_size": {
      "type": "integer",
      "description": "Maximum cudagraph capture size"
    },
    "cudagraph_mode": {
      "type": "string",
      "description": "CUDA graph mode configuration"
    },
    "cudagraph_num_of_warmups": {
      "type": "integer",
      "default": 0,
      "description": "Number of CUDA graph warmup iterations"
    },
    "cudagraph_copy_inputs": {
      "type": "boolean",
      "description": "Copy CUDA graph inputs"
    },
    "cudagraph_specialize_lora": {
      "type": "boolean",
      "description": "Specialize CUDA graphs for LoRA"
    },
    "speculative_config": {
      "type": ["string", "object"],
      "description": "Speculative decoding configuration"
    },
    "kv_transfer_config": {
      "type": ["string", "object"],
      "description": "Distributed KV cache transfer config"
    },
    "kv_events_config": {
      "type": ["string", "object"],
      "description": "Event publishing configuration"
    },
    "ec_transfer_config": {
      "type": ["string", "object"],
      "description": "Distributed EC cache transfer config"
    },
    "compilation_config": {
      "type": ["string", "object"],
      "description": "torch.compile and cudagraph configuration"
    },
    "additional_config": {
      "type": "object",
      "default": {},
      "description": "Additional platform-specific configuration"
    },
    "structured_outputs_config": {
      "type": ["string", "object"],
      "description": "Structured outputs configuration"
    },
    "headless": {
      "type": "boolean",
      "default": false,
      "description": "Run in headless mode"
    },
    "api_server_count": {
      "type": "integer",
      "minimum": 1,
      "default": 1,
      "description": "Number of API server processes"
    },
    "disable_log_stats": {
      "type": "boolean",
      "default": false,
      "description": "Disable logging statistics"
    },
    "aggregate_engine_logging": {
      "type": "boolean",
      "default": false,
      "description": "Log aggregate statistics with data parallel"
    },
    "chat_template": {
      "type": "string",
      "description": "Chat template file path or inline template"
    },
    "chat_template_content_format": {
      "type": "string",
      "enum": ["auto", "string", "openai"],
      "default": "auto",
      "description": "Chat template content format"
    },
    "trust_request_chat_template": {
      "type": "boolean",
      "default": false,
      "description": "Trust chat template from request"
    },
    "response_role": {
      "type": "string",
      "default": "assistant",
      "description": "Role name to return for generation"
    },
    "enable_prompt_tokens_details": {
      "type": "boolean",
      "default": false,
      "description": "Enable prompt tokens details in response"
    },
    "enable_auto_tool_choice": {
      "type": "boolean",
      "default": false,
      "description": "Enable auto tool choice"
    },
    "exclude_tools_when_tool_choice_none": {
      "type": "boolean",
      "default": false,
      "description": "Exclude tools when tool_choice is none"
    },
    "tool_call_parser": {
      "type": "string",
      "description": "Tool call parser to use"
    },
    "tool_parser_plugin": {
      "type": "string",
      "default": "",
      "description": "Tool parser plugin path"
    },
    "tool_server": {
      "type": "string",
      "description": "Tool server host:port pairs"
    },
    "enable_server_load_tracking": {
      "type": "boolean",
      "default": false,
      "description": "Track server_load_metrics in app state"
    },
    "enable_force_include_usage": {
      "type": "boolean",
      "default": false,
      "description": "Include usage on every request"
    },
    "enable_tokenizer_info_endpoint": {
      "type": "boolean",
      "default": false,
      "description": "Enable /get_tokenizer_info endpoint"
    },
    "enable_log_outputs": {
      "type": "boolean",
      "default": false,
      "description": "Log model outputs"
    },
    "log_error_stack": {
      "type": "boolean",
      "default": false,
      "description": "Log stack trace of error responses"
    },
    "tokens_only": {
      "type": "boolean",
      "default": false,
      "description": "Only enable Tokens In<>Out endpoint"
    },
    "preemption_mode": {
      "type": "string",
      "enum": ["swap", "recompute"],
      "description": "Preemption mode during memory shortage"
    },
    "num_scheduler_steps": {
      "type": "integer",
      "minimum": 1,
      "default": 1,
      "description": "Number of scheduler steps"
    },
    "multi_step_stream_outputs": {
      "type": "boolean",
      "default": false,
      "description": "Enable multi-step stream outputs"
    },
    "rope_scaling": {
      "type": "object",
      "description": "RoPE scaling configuration"
    },
    "rope_theta": {
      "type": "number",
      "minimum": 0,
      "description": "RoPE theta parameter"
    },
    "enable_reasoning": {
      "type": "boolean",
      "default": false,
      "description": "Enable reasoning mode"
    },
    "max_seq_len_to_capture": {
      "type": "integer",
      "minimum": 1,
      "default": 8192,
      "description": "Maximum sequence length covered by CUDA graphs"
    }
  },
  "additionalProperties": false
}
